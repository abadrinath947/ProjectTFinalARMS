{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - MNIST Classification Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Necessary Libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, Input, MaxPooling2D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "As we saw in problem 1, regression seems to work fairly well, so let's take a crack at classification using the MNIST dataset.\n",
    "\n",
    "The MNIST dataset is a collection of handwritten digits, from 0 to 9, that has been used as an example dataset to showcase machine learning models' capabilities to interpret human readable text. We will attempt to do the same using a similar model to that which has been shown in problem 1 and a new convolutional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('mnist.npz')\n",
    "\n",
    "# load pre-decided train and test data\n",
    "X_train, y_train = data['x_train'], data['y_train']\n",
    "X_test, y_test = data['x_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Visualization\n",
    "\n",
    "Use matplotlib to visualize any one image of each digit. (Hint: use plt.imshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in sorted(set(y_train)):\n",
    "    \n",
    "    ## YOUR CODE HERE ##\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Preprocessing\n",
    "\n",
    "We need to pre-process the data to make the neural network as effective as possible - we need to normalize the data. There are a lot of simple ways to do this for MNIST. One possible way is to do 0-1 normalization by dividing by 255, which is the highest pixel value. Let us do this to both the X_train and X_test arrays. \n",
    "\n",
    "Additionally, since y has categorical data, let us one hot encode it using keras.utils.to_categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = ...\n",
    "X_test_norm = ...\n",
    "\n",
    "y_train = keras.utils.to_categorical(...)\n",
    "y_test = keras.utils.to_categorical(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Classic MLP\n",
    "\n",
    "Using the note, lecture, and example from problem 1, create a classic MLP with 3 dense hidden layers with 256, 184, and 128 units respectively using 'relu' activation. Then add a dense output layer with 10 units with 'softmax' activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our Sequential model: this will take in a series of \n",
    "# sequential input, hidden and output layers using .add(...)\n",
    "model = Sequential()\n",
    "\n",
    "# hidden layers\n",
    "model.add(Flatten(input_shape = (28, 28)))\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "# add output layer, specifying that we want one output number\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Train the model\n",
    "\n",
    "Now, we just need to use our training data to fit our model. You don't need to write any code for this part, just comment on the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train_norm, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test_norm, y_test, verbose = 0)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Experimentation\n",
    "\n",
    "Not bad! We can classify around 97-98% of the test set correctly using around 500,000 parameters. However, do we need so many parameters? Experiment with using less units in the hidden fully-connected layers. Comment on the performance of the model with 100, 1000, and 10000 parameters.\n",
    "\n",
    "(YOUR ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Convolutional Neural Network\n",
    "\n",
    "We will now train a model more suited to this task, the convolutional neural network, with a similar number of parameters to compare their performance. This section may take a while to run (upwards of 10 minutes), so be patient!\n",
    "\n",
    "The layers that we use in this section are much more complex than the typical Dense (fully-connected) layer. We explain each layer and its purpose below:\n",
    "\n",
    "* Conv2D - This is the convolutional layer, with a particular number of filters. We can customize properties of this layer such as the attached activation function, padding schemes, kernel and stride sizes, and regularization. We typically don't specify all of these parameters as most default to a reasonable setting; the most important parameters are the number of filters and the kernel size.\n",
    "* MaxPooling2D - This is the pooling layer, which compresses the input data by striding and picking the maximal value in each grid specified by the pool size.\n",
    "* Flatten - This flattens a multidimensional tensor into a single dimensional tensor, often near the end of a classification neural network.\n",
    "* Dropout - This serves as a form of regularization that zeroes a proportion of all parameters and rescales the remaining parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our Sequential model: this will take in a series of \n",
    "# sequential input, hidden and output layers using .add(...)\n",
    "model = Sequential()\n",
    "\n",
    "# hidden layers\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same', input_shape = (28, 28, 1)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# add output layer, specifying that we want one output number\n",
    "model.add(Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model can take upwards of 10 minutes\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train_norm.reshape((60000, 28, 28, 1)), y_train, epochs = 2, batch_size = 64);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we evaluate the model, we ought to discuss the performance of the first Dense-based model as compared to this new convolutional neural network. Even though the number of parameters are similar, the time required per epoch is almost a couple of orders of magnitude higher. This fundamentally comes down to the types of tasks that a CPU is equipped to handle efficiently - in this case, the convolution operation is simply just not as efficient enough on a CPU. To solve this issue, we typically use a GPU to train the network since its SIMD (single instruction, multiple data-streams) capabilities far outweigh a conventional CPU.\n",
    "\n",
    "Keeping this in mind, let's evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test_norm.reshape((-1, 28, 28, 1)), y_test, verbose = 0)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) Tradeoff\n",
    "\n",
    "Discuss the tradeoffs of using the first model as opposed to this second model for the MNIST classification problem.\n",
    "\n",
    "(YOUR ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h) Generators\n",
    "\n",
    "Consider the issues that arise when a larger dataset is used with a convolutional neural network framework. Typically, training becomes a longer process, but memory is a source for concern as well. Some datasets in the real world are well into the tens or hundreds of gigabytes or terabytes. We wish to dynamically load our data when it's required, but the current data format requires our data to be passed in as a contiguous array in memory.\n",
    "\n",
    "Let's modify that by using Keras' inbuilt support for Python generators as data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same model as previously, but retrain it using our custom generator\n",
    "def data_gen(X_train, y_train):\n",
    "    while True:\n",
    "        for i in range(0, len(X_train), 32):\n",
    "            yield X_train[i:i+32], y_train[i:i+32]\n",
    "\n",
    "model.fit(data_gen(X_train_norm.reshape((60000, 28, 28, 1)), y_train), steps_per_epoch = len(X_train) // 500);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
