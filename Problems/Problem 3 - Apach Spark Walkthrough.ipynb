{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Apache Spark Walkthrough\n",
    "\n",
    "In this problem, we'll be working with california housing data and using Spark to do parrallelized Linear Regression on some of its columns. The results aren't very accurate at all, but it's a good introduction on various functionalities that Spark has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Data Set\n",
    "\n",
    "The California Housing data set appeared in a 1997 paper titled *Sparse Spatial Autoregressions*, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\n",
    "\n",
    "The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n",
    "\n",
    "These spatial data contain 20,640 observations on housing prices with 9 economic variables:\n",
    "\n",
    "<p style=\"text-align: justify;\"></p>\n",
    "<pre><strong>Longitude:</strong>refers to the angular distance of a geographic place north or south of the earth’s equator for each block group\n",
    "<strong>Latitude :</strong>refers to the angular distance of a geographic place east or west of the earth’s equator for each block group\n",
    "<strong>Housing Median Age:</strong>is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values\n",
    "<strong>Total Rooms:</strong>is the total number of rooms in the houses per block group\n",
    "<strong>Total Bedrooms:</strong>is the total number of bedrooms in the houses per block group\n",
    "<strong>Population:</strong>is the number of inhabitants of a block group\n",
    "<strong>Households:</strong>refers to units of houses and their occupants per block group\n",
    "<strong>Median Income:</strong>is used to register the median income of people that belong to a block group\n",
    "<strong>Median House Value:</strong>is the dependent variable and refers to the median house value per block group\n",
    "</pre>\n",
    "\n",
    "What's more, we also learn that all the block groups have zero entries for the independent and dependent variables have been excluded from the data.\n",
    "\n",
    "The Median house value is the dependent variable and will be assigned the role of the target variable in our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Linear-Regression-California-Housing\").getOrCreate()\n",
    "path = '../input/hausing-data/cal_housing.data'\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"long\", FloatType(), nullable=True),\n",
    "    StructField(\"lat\", FloatType(), nullable=True),\n",
    "    StructField(\"medage\", FloatType(), nullable=True),\n",
    "    StructField(\"totrooms\", FloatType(), nullable=True),\n",
    "    StructField(\"totbdrms\", FloatType(), nullable=True),\n",
    "    StructField(\"pop\", FloatType(), nullable=True),\n",
    "    StructField(\"houshlds\", FloatType(), nullable=True),\n",
    "    StructField(\"medinc\", FloatType(), nullable=True),\n",
    "    StructField(\"medhv\", FloatType(), nullable=True)]\n",
    ")\n",
    "\n",
    "housing_df = spark.read.csv(path=path, schema=schema).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A: Basic Spark Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Display the first five rows of the Spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Create a new 1x1 dataframe, result, which contains the average of the population column\n",
    "\n",
    "Your result should be around 1425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ## YOUR CODE HERE ##\n",
    "result.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Save the pandas version of housing_df as pandas_housing_df\n",
    "\n",
    "Feel free to lookup documentation on how this is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_housing_df = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks if your code worked by plotting median age\n",
    "plt.hist(pandas_housing_df['medage'])\n",
    "plt.xlabel('Median Age')\n",
    "plt.ylabel('Number of Houses')\n",
    "plt.title('Histogram of Median Ages in California Houses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B: Basic Spark Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) First we define a set of feature columns that we would like to use as an input. \n",
    "Right now, lets go with Median Age, Total Bedrooms, Median Income, and Total Rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Use VectorAssembler to create a column names \"features\" which contains the desired features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(## YOUR CODE HERE ##)\n",
    "assembled_df = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Randomly split the data into 80% train data and 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) Set linearModel equal to the result of fitting the given linear regression on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = (LinearRegression(featuresCol='features', labelCol=\"medhv\", predictionCol='predmedhv', \n",
    "                               maxIter=10, regParam=0.3, elasticNetParam=0.8, standardization=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearModel = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h) Use the linearModel to predict on the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Print root mean squared error for the linear model\n",
    "\n",
    "If you did everything correctly, the MSE should be around 80,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ## YOUR CODE HERE ##\n",
    "print(\"MSE:\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (j) Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREDITS:\n",
    "THIS NOTEBOOK IS HEAVILY INSPIRED BY THE ONE BY FATMAKURSUN WHICH YOU CAN FIND HERE https://www.kaggle.com/fatmakursun/pyspark-ml-tutorial-for-beginners. SOME BLOCKS OF CODE, FOR EXAMPLE LOADING THE DATASET, ARE TAKEN DIRECTLY FROM IT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
