{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem 3 - Apache Spark Walkthrough\n\nREMEMBER TO SWITCH OVER TO KAGGLE AT https://www.kaggle.com/mehulraheja/keras-apache-spark-problem-3\n\nIn this problem, we'll be working with california housing data and using Spark to do parrallelized Linear Regression on some of its columns. The results aren't very accurate at all, but it's a good introduction on various functionalities that Spark has."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Housing Data Set\n\nThe California Housing data set appeared in a 1997 paper titled *Sparse Spatial Autoregressions*, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\n\nThe data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n\nThese spatial data contain 20,640 observations on housing prices with 9 economic variables:\n\n<p style=\"text-align: justify;\"></p>\n<pre><strong>Longitude:</strong>refers to the angular distance of a geographic place north or south of the earth’s equator for each block group\n<strong>Latitude :</strong>refers to the angular distance of a geographic place east or west of the earth’s equator for each block group\n<strong>Housing Median Age:</strong>is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values\n<strong>Total Rooms:</strong>is the total number of rooms in the houses per block group\n<strong>Total Bedrooms:</strong>is the total number of bedrooms in the houses per block group\n<strong>Population:</strong>is the number of inhabitants of a block group\n<strong>Households:</strong>refers to units of houses and their occupants per block group\n<strong>Median Income:</strong>is used to register the median income of people that belong to a block group\n<strong>Median House Value:</strong>is the dependent variable and refers to the median house value per block group\n</pre>\n\nWhat's more, we also learn that all the block groups have zero entries for the independent and dependent variables have been excluded from the data.\n\nThe Median house value is the dependent variable and will be assigned the role of the target variable in our ML model."},{"metadata":{"trusted":false},"cell_type":"code","source":"spark = SparkSession.builder.master(\"local[2]\").appName(\"Linear-Regression-California-Housing\").getOrCreate()\npath = '../input/hausing-data/cal_housing.data'\n\nschema = StructType([\n    StructField(\"long\", FloatType(), nullable=True),\n    StructField(\"lat\", FloatType(), nullable=True),\n    StructField(\"medage\", FloatType(), nullable=True),\n    StructField(\"totrooms\", FloatType(), nullable=True),\n    StructField(\"totbdrms\", FloatType(), nullable=True),\n    StructField(\"pop\", FloatType(), nullable=True),\n    StructField(\"houshlds\", FloatType(), nullable=True),\n    StructField(\"medinc\", FloatType(), nullable=True),\n    StructField(\"medhv\", FloatType(), nullable=True)]\n)\n\nhousing_df = spark.read.csv(path=path, schema=schema).cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PART A: Basic Spark Commands"},{"metadata":{},"cell_type":"markdown","source":"## (a) Display the first five rows of the Spark dataframe\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"## YOUR CODE HERE ##","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (b) Create a new 1x1 dataframe, result, which contains the average of the population column\n\nYour result should be around 1425"},{"metadata":{"trusted":false},"cell_type":"code","source":"result = ## YOUR CODE HERE ##\nresult.show(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (c) Save the pandas version of housing_df as pandas_housing_df\n\nFeel free to lookup documentation on how this is done"},{"metadata":{"trusted":false},"cell_type":"code","source":"pandas_housing_df = ## YOUR CODE HERE ##","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checks if your code worked by plotting median age\nplt.hist(pandas_housing_df['medage'])\nplt.xlabel('Median Age')\nplt.ylabel('Number of Houses')\nplt.title('Histogram of Median Ages in California Houses')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PART B: Basic Spark Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"## (d) First we define a set of feature columns that we would like to use as an input. \nRight now, lets go with Median Age, Total Bedrooms, Median Income, and Total Rooms"},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_cols = ## YOUR CODE HERE ##","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (e) Use VectorAssembler to create a column names \"features\" which contains the desired features"},{"metadata":{"trusted":false},"cell_type":"code","source":"assembler = VectorAssembler(## YOUR CODE HERE ##)\nassembled_df = ## YOUR CODE HERE ##","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"assembled_df.show(10, truncate=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (f) Randomly split the data into 80% train data and 20% testing data"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data, test_data = ## YOUR CODE HERE ##","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data.show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (g) Set linearModel equal to the result of fitting the given linear regression on the training data"},{"metadata":{"trusted":false},"cell_type":"code","source":"lr = (LinearRegression(featuresCol='features', labelCol=\"medhv\", predictionCol='predmedhv', \n                               maxIter=10, regParam=0.3, elasticNetParam=0.8, standardization=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"linearModel = ## YOUR CODE HERE ##","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (h) Use the linearModel to predict on the test_data"},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions = ## YOUR CODE HERE ##","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions.show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (i) Print root mean squared error for the linear model\n\nIf you did everything correctly, the MSE should be around 80,000. "},{"metadata":{"trusted":false},"cell_type":"code","source":"mse = ## YOUR CODE HERE ##\nprint(\"MSE:\",mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (j) Stop Spark"},{"metadata":{"trusted":false},"cell_type":"code","source":"## YOUR CODE HERE ##","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CREDITS:\nTHIS NOTEBOOK IS HEAVILY INSPIRED BY THE ONE BY FATMAKURSUN WHICH YOU CAN FIND HERE https://www.kaggle.com/fatmakursun/pyspark-ml-tutorial-for-beginners. SOME BLOCKS OF CODE, FOR EXAMPLE LOADING THE DATASET, ARE TAKEN DIRECTLY FROM IT."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}