{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Now, we'll try to use spark functions to improve the quality of our predictions on the california housing data. For convenience, much of the code to load data and run the model is copied from the previous part. This activity is pretty free form and will be graded for effort instead of accuracy. Here are some ideas of feature engineering you can do to improve the quality of your predictions:\n",
    "\n",
    "1. Try different types of regression (which can be found here) https://spark.apache.org/docs/2.1.1/ml-classification-regression.html\n",
    "2. Think of other features which may be indicative of this data. Some of the most important ones could be rooms, population, or bedrooms per household (which is more helpful than rooms, populations, or bedrooms per block group)\n",
    "3. Try to scale the features using StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Data Set\n",
    "\n",
    "The California Housing data set appeared in a 1997 paper titled *Sparse Spatial Autoregressions*, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\n",
    "\n",
    "The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n",
    "\n",
    "These spatial data contain 20,640 observations on housing prices with 9 economic variables:\n",
    "\n",
    "<p style=\"text-align: justify;\"></p>\n",
    "<pre><strong>Longitude:</strong>refers to the angular distance of a geographic place north or south of the earth’s equator for each block group\n",
    "<strong>Latitude :</strong>refers to the angular distance of a geographic place east or west of the earth’s equator for each block group\n",
    "<strong>Housing Median Age:</strong>is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values\n",
    "<strong>Total Rooms:</strong>is the total number of rooms in the houses per block group\n",
    "<strong>Total Bedrooms:</strong>is the total number of bedrooms in the houses per block group\n",
    "<strong>Population:</strong>is the number of inhabitants of a block group\n",
    "<strong>Households:</strong>refers to units of houses and their occupants per block group\n",
    "<strong>Median Income:</strong>is used to register the median income of people that belong to a block group\n",
    "<strong>Median House Value:</strong>is the dependent variable and refers to the median house value per block group\n",
    "</pre>\n",
    "\n",
    "What's more, we also learn that all the block groups have zero entries for the independent and dependent variables have been excluded from the data.\n",
    "\n",
    "The Median house value is the dependent variable and will be assigned the role of the target variable in our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (3.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Linear-Regression-California-Housing\").getOrCreate()\n",
    "path = '../input/hausing-data/cal_housing.data'\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"long\", FloatType(), nullable=True),\n",
    "    StructField(\"lat\", FloatType(), nullable=True),\n",
    "    StructField(\"medage\", FloatType(), nullable=True),\n",
    "    StructField(\"totrooms\", FloatType(), nullable=True),\n",
    "    StructField(\"totbdrms\", FloatType(), nullable=True),\n",
    "    StructField(\"pop\", FloatType(), nullable=True),\n",
    "    StructField(\"houshlds\", FloatType(), nullable=True),\n",
    "    StructField(\"medinc\", FloatType(), nullable=True),\n",
    "    StructField(\"medhv\", FloatType(), nullable=True)]\n",
    ")\n",
    "\n",
    "housing_df = spark.read.csv(path=path, schema=schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+--------+--------+------+--------+------+--------+\n",
      "|   long|  lat|medage|totrooms|totbdrms|   pop|houshlds|medinc|   medhv|\n",
      "+-------+-----+------+--------+--------+------+--------+------+--------+\n",
      "|-122.23|37.88|  41.0|   880.0|   129.0| 322.0|   126.0|8.3252|452600.0|\n",
      "|-122.22|37.86|  21.0|  7099.0|  1106.0|2401.0|  1138.0|8.3014|358500.0|\n",
      "|-122.24|37.85|  52.0|  1467.0|   190.0| 496.0|   177.0|7.2574|352100.0|\n",
      "|-122.25|37.85|  52.0|  1274.0|   235.0| 558.0|   219.0|5.6431|341300.0|\n",
      "|-122.25|37.85|  52.0|  1627.0|   280.0| 565.0|   259.0|3.8462|342200.0|\n",
      "+-------+-----+------+--------+--------+------+--------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maybe create some features here ##\n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ## YOUR CODE HERE ##\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(housing_df)\n",
    "\n",
    "##Maybe scale your features here ##\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "train_data, test_data = assembled_df.randomSplit([.8,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE: Fit a regression model to the data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 73605.18643688427\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"medhv\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "mse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"MSE:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREDITS:\n",
    "THIS NOTEBOOK IS HEAVILY INSPIRED BY THE ONE BY FATMAKURSUN WHICH YOU CAN FIND HERE https://www.kaggle.com/fatmakursun/pyspark-ml-tutorial-for-beginners. SOME BLOCKS OF CODE, FOR EXAMPLE LOADING THE DATASET, ARE TAKEN DIRECTLY FROM IT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
