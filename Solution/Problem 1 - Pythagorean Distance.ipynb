{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Pythagorean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Necessary Libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, Input, MaxPooling2D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We will explore the most basic neural network with a dummy dataset of Pythagorean relations in a triangle. Mathematically speaking, we wish to model the following relationship using our neural network:\n",
    "\n",
    "$f(x_1, x_2) = \\sqrt{x_1^2 + x_2^2}$\n",
    "\n",
    "Clearly, ordinary linear regression would be unable to capture the non-linearities introduced by these operations. However, using the non-linear functions presented by neural networks in Keras, we can effectively model this relationship!\n",
    "\n",
    "To do this, we start by generating random values for our $x_1$ and $x_2$, then computing $f(x_1, x_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates random x and the exact y's\n",
    "X = np.random.rand(100000,2) * 10\n",
    "y = np.array([np.sqrt(X[:,0]**2 + X[:,1]**2)]).T\n",
    "\n",
    "# adds random noise to our training observations depending\n",
    "# on the standard deviation stdev\n",
    "stdev = 0.05\n",
    "y += np.random.normal(scale = stdev, size = y.shape)\n",
    "\n",
    "# training-test split \n",
    "split_prop = int(0.9 * len(X))\n",
    "inds = np.random.permutation(len(X))\n",
    "X_train, X_test = X[inds][:split_prop], X[inds][split_prop:]\n",
    "y_train, y_test = y[inds][:split_prop], y[inds][split_prop:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that `X_train[i]` corresponds with a pair of $x_1$ and $x_2$ such that `y_train[i]` corresponds with $f(x_1, x_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.24873679, 4.37087753],\n",
       "        [3.46083666, 1.54525046],\n",
       "        [0.00971231, 2.5946869 ],\n",
       "        ...,\n",
       "        [9.30546637, 7.26111725],\n",
       "        [2.44373281, 8.85285077],\n",
       "        [6.17355592, 8.30607113]]),\n",
       " array([[ 6.8240648 ],\n",
       "        [ 3.73969882],\n",
       "        [ 2.63143795],\n",
       "        ...,\n",
       "        [11.77674043],\n",
       "        [ 9.19321808],\n",
       "        [10.31163437]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if stdev == 0:\n",
    "    assert np.isclose(X_train[0][0] ** 2 + X_train[0][1] ** 2, y_train[0] ** 2,rtol = 1,atol = 3 * stdev), \"Something is wrong!\"\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct our first Keras model step-by-step. The interface that we will use for this first model is the Sequential interface, where we `add()` each layer (i.e. fully connected layer, convolutional layer, pooling layer) separately. We are able to optionally specify a simple activation function as a parameter to these layers (note that more complex activation functions like LeakyRELU that rely on states require separate layers).\n",
    "\n",
    "A simple visualization of the layers can be achieved through the `summary()` function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize our Sequential model: this will take in a series of \n",
    "# sequential input, hidden and output layers using .add(...)\n",
    "model = Sequential()\n",
    "\n",
    "# add input layer where the expected input shape is a 1x2 row vector\n",
    "model.add(Dense(units = 2, activation='relu', input_shape=[2]))\n",
    "\n",
    "# add hidden layers where the activation functions are non-linear so as\n",
    "# to help us capture the non-linearities in Pythagorean relation\n",
    "model.add(Dense(units = 10, activation='tanh'))\n",
    "model.add(Dense(units = 5, activation='exponential'))\n",
    "\n",
    "# add output layer, specifying that we want one output number\n",
    "model.add(Dense(units = 1, activation='exponential'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and test this model. We need to specify a few arguments such as the number of epochs (thought of as the number of passes over the training data), the loss function, and the optimizer. For the most part, we will use optimizers like Adam, RMSProp, or SGD - more advanced optimizers exist, but typically these 3 will be used the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2250/2250 [==============================] - 3s 1ms/step - loss: 0.4968 - val_loss: 0.0493\n",
      "Epoch 2/10\n",
      "2250/2250 [==============================] - 2s 1ms/step - loss: 0.0335 - val_loss: 0.0180\n",
      "Epoch 3/10\n",
      "2250/2250 [==============================] - 3s 1ms/step - loss: 0.0148 - val_loss: 0.0101\n",
      "Epoch 4/10\n",
      "2250/2250 [==============================] - 2s 1ms/step - loss: 0.0089 - val_loss: 0.0061\n",
      "Epoch 5/10\n",
      "2250/2250 [==============================] - 2s 1ms/step - loss: 0.0070 - val_loss: 0.0063\n",
      "Epoch 6/10\n",
      "2250/2250 [==============================] - 2s 1ms/step - loss: 0.0061 - val_loss: 0.0044\n",
      "Epoch 7/10\n",
      "2250/2250 [==============================] - 3s 1ms/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 8/10\n",
      "2250/2250 [==============================] - 2s 1ms/step - loss: 0.0054 - val_loss: 0.0043\n",
      "Epoch 9/10\n",
      "2250/2250 [==============================] - 2s 1ms/step - loss: 0.0052 - val_loss: 0.0040\n",
      "Epoch 10/10\n",
      "2250/2250 [==============================] - 2s 1ms/step - loss: 0.0052 - val_loss: 0.0066\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # how can we most appropriately choose the number of epochs?\n",
    "loss = 'mse' # try 'mae', 'mean_absolute_percentage_error', ... how can we most appropriately choose a loss function?\n",
    "\n",
    "# compile the model by specifying an optimization technique and loss function\n",
    "model.compile(optimizer='Adam', loss=loss)\n",
    "\n",
    "# train the model on our training data\n",
    "model.fit(X_train, y_train, epochs = epochs, validation_split = 0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.006627020552956391\n",
      "Test Error: 0.0066704840487086785\n"
     ]
    }
   ],
   "source": [
    "# find prediction error\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "print(\"Training Error:\", mse(y_train, y_pred_train))\n",
    "print(\"Test Error:\", mse(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3.5349727],\n",
       "        [ 5.1418886],\n",
       "        [ 9.31138  ],\n",
       "        ...,\n",
       "        [ 3.2122893],\n",
       "        [ 4.6933117],\n",
       "        [11.452698 ]], dtype=float32),\n",
       " array([[ 3.48411708],\n",
       "        [ 5.11029192],\n",
       "        [ 9.20096614],\n",
       "        ...,\n",
       "        [ 3.19650241],\n",
       "        [ 4.65777527],\n",
       "        [11.31280545]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Remember to recompile the model everytime you change something\n",
    "\n",
    "#####  (a) What are the shapes of the inputs and outputs for this model?\n",
    "\n",
    "_*The shape of X_train is (90000, 2) and the shape of y_train is (90000, 1)*_\n",
    "\n",
    "#####  (b) What happens to the performance of the model as you increase the standard deviation of the noise?\n",
    "\n",
    "_*The model's levels of loss increase proportionally, but as long as the standard deviation is within reasonable bounds, the model still seems to be useful*_\n",
    "\n",
    "#####  (c) What happens if you add more layers? What happens if you remove some of the layers?\n",
    "\n",
    "_*Adding more layers makes it so the training time increases fairly drastically, although it allows training error to somewhat drop (even though test error doesn't drop much). Removing layers makes the training much faster, but doesn't allow the model to fit to the data entirely.*_\n",
    "\n",
    "##### (d) How does the number of epochs affect the training and test error?\n",
    "\n",
    "_*Having extremeley few epochs makes the model underfit (although not by too much since we have a lot of data). Adding a lot more epochs increases the test error while decreasing the training error, although past a certain point the difference becomes pretty unnoticable.*_\n",
    "\n",
    "##### (e) What do you think of the overall performance of this neural network? Would you use this or another approach for this problem?\n",
    "\n",
    "_*Neural networks are nice because you don't need to do the feature engineering yourself (i.e saying that the output is proportional to sums of squares of inputs). However, if we spent a little more time observing this data and doing that feature engineering, we could have just as easily reduced the problem to something that could be solved by linear regression. Neural networks didn't perform terribly, but, especially for a problem this simple, traditional ML methods are much more effective.*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
